
import streamlit as st
from models.chatglm import call_chatglm
from models.qwen import call_qwen
from models.baichuan import call_baichuan
from models.yi import call_yi
from models.gpt import call_gpt
from models.deepseek import call_deepseek
import json
import os

st.set_page_config(page_title="å¤šæ¨¡å‹ä¸­æ–‡å†™ä½œåŠ©æ‰‹", layout="wide")
st.title("ğŸ“ å¤šæ¨¡å‹ä¸­æ–‡å†™ä½œåŠ©æ‰‹")

st.markdown("""
è¾“å…¥æçº²åï¼Œå°†åŒæ—¶è°ƒç”¨å¤šä¸ªä¸­æ–‡å¤§æ¨¡å‹ç”Ÿæˆå†™ä½œå†…å®¹ï¼Œä¾›ä½ æ¯”å¯¹é€‰æ‹©ã€‚
""")

prompt_template = st.selectbox("ğŸ“‹ é€‰æ‹©å†™ä½œæ¨¡æ¿ï¼š", ["é€šç”¨", "å­¦æœ¯è®ºæ–‡", "å·¥ä½œæ±‡æŠ¥", "å¿ƒå¾—æ€»ç»“"], index=0)
custom_prefix = st.text_area("âœï¸ å¯é€‰ï¼šè‡ªå®šä¹‰æç¤ºè¯å‰ç¼€ï¼ˆç•™ç©ºåˆ™ä½¿ç”¨æ¨¡æ¿é»˜è®¤ï¼‰")

template_prefix = {
    "é€šç”¨": "è¯·æ ¹æ®ä»¥ä¸‹æçº²ç”Ÿæˆä¸€æ®µè¿è´¯çš„ä¸­æ–‡å†…å®¹ï¼š",
    "å­¦æœ¯è®ºæ–‡": "è¯·æ ¹æ®ä»¥ä¸‹æçº²ï¼Œç”Ÿæˆä¸€æ®µç¬¦åˆå­¦æœ¯è®ºæ–‡é£æ ¼çš„æ–‡å­—ï¼š",
    "å·¥ä½œæ±‡æŠ¥": "è¯·æ ¹æ®ä»¥ä¸‹æçº²ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„æ¸…æ™°çš„å·¥ä½œæ±‡æŠ¥å†…å®¹ï¼š",
    "å¿ƒå¾—æ€»ç»“": "è¯·æ ¹æ®ä»¥ä¸‹æçº²ï¼Œç”Ÿæˆä¸€æ®µä¸ªäººå¿ƒå¾—æ€»ç»“ç±»å‹çš„å†…å®¹ï¼š"
}

topic = st.text_area("âœï¸ è¯·è¾“å…¥å†™ä½œæçº²ï¼š", height=200)

models_selected = st.multiselect(
    "ğŸ§  é€‰æ‹©è¦è°ƒç”¨çš„å¤§æ¨¡å‹ï¼š",
    ["ChatGLM", "Qwen", "Baichuan", "Yi", "DeepSeek", "GPT"],
    default=["ChatGLM", "Qwen", "DeepSeek"]
)

gpt_model = "gpt-3.5-turbo"
if "GPT" in models_selected:
    gpt_model = st.radio("âš™ï¸ GPT ç‰ˆæœ¬ï¼š", ["gpt-3.5-turbo", "gpt-4o"], horizontal=True)

history_file = "history.json"
history = []
if os.path.exists(history_file):
    with open(history_file, "r", encoding="utf-8") as f:
        history = json.load(f)

st.markdown("---")
st.subheader("ğŸ“œ å†å²è®°å½•")
if history:
    for i, record in enumerate(reversed(history[-5:])):
        with st.expander(f"ğŸ“„ ç¬¬ {len(history) - i} æ¡è®°å½•ï¼ˆæ¨¡æ¿ï¼š{record['template']}ï¼‰"):
            st.code(record['prompt'], language="markdown")
            if st.button(f"ğŸ” é‡æ–°ç”Ÿæˆç¬¬ {len(history) - i} æ¡", key=f"regen_{i}"):
                topic = record['prompt'].split("\n", 1)[-1]
                custom_prefix = record['prompt'].split("\n", 1)[0]
                prompt_template = record['template']
                st.experimental_rerun()
else:
    st.info("æš‚æ— å†å²è®°å½•ã€‚")

if st.button("ğŸš€ å¼€å§‹ç”Ÿæˆ") and topic.strip():
    st.info("æ­£åœ¨ç”Ÿæˆå†…å®¹ï¼Œè¯·ç¨å€™...")
    results = {}
    prefix = custom_prefix if custom_prefix.strip() else template_prefix[prompt_template]
    full_prompt = f"{prefix}\n{topic}"

    if "ChatGLM" in models_selected:
        results["ChatGLM"] = call_chatglm(full_prompt)
    if "Qwen" in models_selected:
        results["Qwen"] = call_qwen(full_prompt)
    if "Baichuan" in models_selected:
        results["Baichuan"] = call_baichuan(full_prompt)
    if "Yi" in models_selected:
        results["Yi"] = call_yi(full_prompt)
    if "DeepSeek" in models_selected:
        results["DeepSeek"] = call_deepseek(full_prompt)
    if "GPT" in models_selected:
        content, usage = call_gpt(full_prompt, model=gpt_model, return_usage=True)
        results[f"GPT ({gpt_model})"] = content
        st.markdown(f"ğŸ”¢ GPT Token ä½¿ç”¨é‡ï¼š**{usage}** tokens")

    for model, content in results.items():
        st.subheader(f"ğŸ“Œ {model} è¾“å‡ºç»“æœ")
        st.write(content)

    history.append({
        "prompt": full_prompt,
        "template": prompt_template,
        "results": results
    })
    with open(history_file, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    st.success("âœ… å†™ä½œå®Œæˆï¼Œå†å²è®°å½•å·²ä¿å­˜ã€‚")
else:
    st.caption("è¯·å…ˆè¾“å…¥æçº²å¹¶é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ¨¡å‹ã€‚")
