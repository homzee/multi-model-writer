import streamlit as st
from models.chatglm import call_chatglm
from models.qwen import call_qwen
from models.baichuan import call_baichuan
from models.yi import call_yi
from models.gpt import call_gpt
from models.deepseek import call_deepseek
import json
import os

st.set_page_config(page_title="å¤šæ¨¡å‹ä¸­æ–‡å†™ä½œåŠ©æ‰‹", layout="wide")
st.title("ğŸ“ å¤šæ¨¡å‹ä¸­æ–‡å†™ä½œåŠ©æ‰‹")

st.markdown("""è¾“å…¥æçº²åï¼Œå°†åŒæ—¶è°ƒç”¨å¤šä¸ªä¸­æ–‡å¤§æ¨¡å‹ç”Ÿæˆå†™ä½œå†…å®¹ï¼Œä¾›ä½ æ¯”å¯¹é€‰æ‹©ã€‚""")

# åˆå§‹åŒ–çŠ¶æ€
if "topic" not in st.session_state:
    st.session_state.topic = ""
if "custom_prefix" not in st.session_state:
    st.session_state.custom_prefix = ""
if "prompt_template" not in st.session_state:
    st.session_state.prompt_template = "é€šç”¨"
if "force_generate" not in st.session_state:
    st.session_state.force_generate = False

template_prefix = {
    "é€šç”¨": "è¯·æ ¹æ®ä»¥ä¸‹æçº²ç”Ÿæˆä¸€æ®µè¿è´¯çš„ä¸­æ–‡å†…å®¹ï¼š",
    "å­¦æœ¯è®ºæ–‡": "è¯·æ ¹æ®ä»¥ä¸‹æçº²ï¼Œç”Ÿæˆä¸€æ®µç¬¦åˆå­¦æœ¯è®ºæ–‡é£æ ¼çš„æ–‡å­—ï¼š",
    "å·¥ä½œæ±‡æŠ¥": "è¯·æ ¹æ®ä»¥ä¸‹æçº²ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„æ¸…æ™°çš„å·¥ä½œæ±‡æŠ¥å†…å®¹ï¼š",
    "å¿ƒå¾—æ€»ç»“": "è¯·æ ¹æ®ä»¥ä¸‹æçº²ï¼Œç”Ÿæˆä¸€æ®µä¸ªäººå¿ƒå¾—æ€»ç»“ç±»å‹çš„å†…å®¹ï¼š"
}

# UI æ§ä»¶
st.session_state.prompt_template = st.selectbox(
    "ğŸ“‹ é€‰æ‹©å†™ä½œæ¨¡æ¿ï¼š",
    list(template_prefix.keys()),
    index=list(template_prefix.keys()).index(st.session_state.prompt_template)
)
st.session_state.custom_prefix = st.text_area("âœï¸ å¯é€‰ï¼šè‡ªå®šä¹‰æç¤ºè¯å‰ç¼€ï¼ˆç•™ç©ºåˆ™ä½¿ç”¨æ¨¡æ¿é»˜è®¤ï¼‰", value=st.session_state.custom_prefix)
st.session_state.topic = st.text_area("âœï¸ è¯·è¾“å…¥å†™ä½œæçº²ï¼š", height=200, value=st.session_state.topic)

models_selected = st.multiselect(
    "ğŸ§  é€‰æ‹©è¦è°ƒç”¨çš„å¤§æ¨¡å‹ï¼š",
    ["ChatGLM", "Qwen", "Baichuan", "Yi", "DeepSeek", "GPT"],
    default=["ChatGLM", "Qwen", "DeepSeek"]
)

gpt_model = "gpt-3.5-turbo"
if "GPT" in models_selected:
    gpt_model = st.radio("âš™ï¸ GPT ç‰ˆæœ¬ï¼š", ["gpt-3.5-turbo", "gpt-4o"], horizontal=True)

# å†å²è®°å½•
history_file = "history.json"
history = []
if os.path.exists(history_file):
    with open(history_file, "r", encoding="utf-8") as f:
        history = json.load(f)

st.markdown("---")
st.subheader("ğŸ“œ å†å²è®°å½•")
if history:
    for i, record in enumerate(reversed(history[-5:])):
        with st.expander(f"ğŸ“„ ç¬¬ {len(history) - i} æ¡è®°å½•ï¼ˆæ¨¡æ¿ï¼š{record['template']}ï¼‰"):
            st.code(record['prompt'], language="markdown")
            if st.button(f"ğŸ” é‡æ–°ç”Ÿæˆç¬¬ {len(history) - i} æ¡", key=f"regen_{i}"):
                st.session_state.topic = record['prompt'].split("\n", 1)[-1]
                st.session_state.custom_prefix = record['prompt'].split("\n", 1)[0]
                st.session_state.prompt_template = record['template']
                st.session_state.force_generate = True

else:
    st.info("æš‚æ— å†å²è®°å½•ã€‚")

# æå‰è§¦å‘ç”Ÿæˆï¼ˆå¦‚æ¥è‡ªå†å²è®°å½•ï¼‰
if st.session_state.force_generate:
    st.session_state.force_generate = False
    st.experimental_rerun()

# ç”ŸæˆæŒ‰é’®
if st.button("ğŸš€ å¼€å§‹ç”Ÿæˆ") and st.session_state.topic.strip():
    st.info("æ­£åœ¨ç”Ÿæˆå†…å®¹ï¼Œè¯·ç¨å€™...")
    results = {}
    prefix = st.session_state.custom_prefix.strip() or template_prefix[st.session_state.prompt_template]
    full_prompt = f"{prefix}\n{st.session_state.topic}"

    if "ChatGLM" in models_selected:
        results["ChatGLM"] = call_chatglm(full_prompt)
    if "Qwen" in models_selected:
        results["Qwen"] = call_qwen(full_prompt)
    if "Baichuan" in models_selected:
        results["Baichuan"] = call_baichuan(full_prompt)
    if "Yi" in models_selected:
        results["Yi"] = call_yi(full_prompt)
    if "DeepSeek" in models_selected:
        results["DeepSeek"] = call_deepseek(full_prompt)
    if "GPT" in models_selected:
        content, usage = call_gpt(full_prompt, model=gpt_model, return_usage=True)
        results[f"GPT ({gpt_model})"] = content
        st.markdown(f"ğŸ”¢ GPT Token ä½¿ç”¨é‡ï¼š**{usage}** tokens")

    st.subheader("ğŸ“Œ æ¨¡å‹ç”Ÿæˆå†…å®¹å¯¹æ¯”")
    cols = st.columns(len(results))
    for idx, (model, content) in enumerate(results.items()):
        with cols[idx]:
            st.markdown(f"**{model}**")
            st.write(content)

    history.append({
        "prompt": full_prompt,
        "template": st.session_state.prompt_template,
        "results": results
    })
    with open(history_file, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

    st.success("âœ… å†™ä½œå®Œæˆï¼Œå†å²è®°å½•å·²ä¿å­˜ã€‚")
else:
    st.caption("è¯·å…ˆè¾“å…¥æçº²å¹¶é€‰æ‹©æ¨¡å‹")
